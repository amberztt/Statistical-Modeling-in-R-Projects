---
title: "Stat 420: Simulation Project"
author: "Tiantian Zhang  (NetId: tz8)"
date: '06/23/2019'
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

***

This simulation project consists of two indepedent simulation studies which focus on significance of regression and using RMSE for selection respectively.

```{r}
library(knitr)
library(ggplot2)
```


## Simulation Study 1: Significance of Regression

- **Introduction**

  In the first simulation study, we are going to investigate the significance of regression test. We will run the simulation using the following two different models:
  
   1. The "**significant**" model
   \[
    Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i
    \]
    
   Where $\epsilon_i \sim N(0, \sigma ^ 2)$ and 
   - $\beta_0 = 3$
   - $\beta_1 = 1$
   - $\beta_2 = 1$
   - $\beta_3 = 1$
   
   2. The "**non-significant**" model
   \[
    Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i
    \]
    
   Where $\epsilon_i \sim N(0, \sigma ^ 2)$ and 
   - $\beta_0 = 3$
   - $\beta_1 = 0$
   - $\beta_2 = 0$
   - $\beta_3 = 0$
   
   We will use sample size of 25 and three levels of noise for both models.
   - n = 25
   - $\sigma \in (1,5,10)$
   
   Through simulation, we are going to obtain the empirical distribution for the following values, for each of the three values of $\sigma$, for both models.
   - The $F$ **statistic** for the significance of regression test.
   - The **p-value** for the significance of regression test
   - $R ^ 2$
   
   The simulation process will be repeated for 2500 times for each model and $\sigma$ combination.
   
   We will use the data found in [`study_1.csv`](study_1.csv) for the values of the predictors. These data are kept constant for the entirety of this study. The y values in this data are a blank placeholder.


   After getting the results, we will be discussing the following questions:
    
   - Do we know the true distribution of any of these values?
   -How do the empirical distributions from the simulations compare to the true distributions? 
   - How are $R ^ 2$ and $\sigma$ related? Is the relationship the same for the significant and non-significant models?
    


- **Methods**

```{r}
#setting seeds
birthday = 19980121
set.seed(birthday)
```

First, we import the data of predictors from the "study_1.csv" file.
  

```{r}
#Import data
library(readr)
study_1 = read.csv("study_1.csv")
```

Fix the parameters for signficant model and non-significant model.

```{r}
#betas for both models with "s" indicating significant model and "ns" indicating non-significant model

#parameters for significant model
beta_0_s = 3
beta_1_s = 1
beta_2_s = 1
beta_3_s = 1

#parameters for non-significant model
beta_0_ns = 3
beta_1_ns = 0
beta_2_ns = 0
beta_3_ns = 0

n = 25
sigma = c(1, 5, 10)

#Inputs for simulation
num_sim = 2500

x1 = study_1$x1
x2 = study_1$x2
x3 = study_1$x3

#Create results containers to store the simulated results 

f_s_sig1 = rep(0, num_sim)
f_ns_sig1 = rep(0, num_sim)
f_s_sig5 = rep(0, num_sim)
f_ns_sig5 = rep(0, num_sim)
f_s_sig10 = rep(0, num_sim)
f_ns_sig10 = rep(0, num_sim)


p_val_s_sig1 = rep(0, num_sim)
p_val_ns_sig1 = rep(0, num_sim)
p_val_s_sig5 = rep(0, num_sim)
p_val_ns_sig5 = rep(0, num_sim)
p_val_s_sig10 = rep(0, num_sim)
p_val_ns_sig10 = rep(0, num_sim)


r2_s_sig1 = rep(0, num_sim)
r2_ns_sig1 = rep(0, num_sim)
r2_s_sig5 = rep(0, num_sim)
r2_ns_sig5 = rep(0, num_sim)
r2_s_sig10 = rep(0, num_sim)
r2_ns_sig10 = rep(0, num_sim)

```


Then, we start the simulation and store the f statistics, p-values and $R^2s$ into the results containers that we just created.

We are going to start simulations for each Ïƒ value separately. We will start with $\sigma = 1$.

```{r}
#Simulation for sigma = 1
for(i in 1: num_sim){
    eps = rnorm(n, mean = 0, sd = 1)
   
    #Simulation for significant model
    y = as.vector(beta_0_s + beta_1_s * x1 + beta_2_s * x2 + beta_3_s * x3 + eps)
    study_1$y = y
    model_s = lm(y ~ x1 + x2 + x3, data = study_1)
    
    f_s_sig1[i] = summary(model_s)$fstatistic[[1]]
    p_val_s_sig1[i] = pf(summary(model_s)$fstatistic[1], df1 = summary(model_s)$fstatistic[2], df2 = summary(model_s)$fstatistic[3], lower.tail = FALSE)
    r2_s_sig1[i] = summary(model_s)$r.squared
    
    #Simulation for non-significant model
    y = as.vector(beta_0_ns + beta_1_ns * x1 + beta_2_ns * x2 + beta_3_ns * x3 + eps)
    study_1$y = y
    model_ns = lm(y ~ x1 + x2 + x3, data = study_1)
    
    f_ns_sig1[i] = summary(model_ns)$fstatistic[[1]]
    p_val_ns_sig1[i] = pf(summary(model_ns)$fstatistic[1], df1 = summary(model_ns)$fstatistic[2], df2 = summary(model_ns)$fstatistic[3], lower.tail = FALSE)
    r2_ns_sig1[i] = summary(model_ns)$r.squared
    
}
```


Then, we run the regression for $\sigma = 5$.

```{r}
#Simulation for sigma = 5
for(i in 1: num_sim){
    eps = rnorm(n, mean = 0, sd = 5)
   
    #Simulation for significant model
    y = as.vector(beta_0_s + beta_1_s * x1 + beta_2_s * x2 + beta_3_s * x3 + eps)
    study_1$y = y
    model_s = lm(y ~ x1 + x2 + x3, data = study_1)
    
    f_s_sig5[i] = summary(model_s)$fstatistic[[1]]
    p_val_s_sig5[i] = pf(summary(model_s)$fstatistic[1], df1 = summary(model_s)$fstatistic[2], df2 = summary(model_s)$fstatistic[3], lower.tail = FALSE)
    r2_s_sig5[i] = summary(model_s)$r.squared
    
    #Simulation for non-significant model
    y = as.vector(beta_0_ns + beta_1_ns * x1 + beta_2_ns * x2 + beta_3_ns * x3 + eps)
    study_1$y = y
    model_ns = lm(y ~ x1 + x2 + x3, data = study_1)
    
    f_ns_sig5[i] = summary(model_ns)$fstatistic[[1]]
    p_val_ns_sig5[i] = pf(summary(model_ns)$fstatistic[1], df1 = summary(model_ns)$fstatistic[2], df2 = summary(model_ns)$fstatistic[3], lower.tail = FALSE)
    r2_ns_sig5[i] = summary(model_ns)$r.squared
    
}

```



Finally, we run the regression for $\sigma = 10$.

```{r}
#Simulation for sigma = 10
for(i in 1: num_sim){
    eps = rnorm(n, mean = 0, sd = 10)
   
    #Simulation for significant model
    y = as.vector(beta_0_s + beta_1_s * x1 + beta_2_s * x2 + beta_3_s * x3 + eps)
    study_1$y = y
    model_s = lm(y ~ x1 + x2 + x3, data = study_1)
    
    f_s_sig10[i] = summary(model_s)$fstatistic[[1]]
    p_val_s_sig10[i] = pf(summary(model_s)$fstatistic[1], df1 = summary(model_s)$fstatistic[2], df2 = summary(model_s)$fstatistic[3], lower.tail = FALSE)
    r2_s_sig10[i] = summary(model_s)$r.squared
    
    #Simulation for non-significant model
    y = as.vector(beta_0_ns + beta_1_ns * x1 + beta_2_ns * x2 + beta_3_ns * x3 + eps)
    study_1$y = y
    model_ns = lm(y ~ x1 + x2 + x3, data = study_1)
    
    f_ns_sig10[i] = summary(model_ns)$fstatistic[[1]]
    p_val_ns_sig10[i] = pf(summary(model_ns)$fstatistic[1], df1 = summary(model_ns)$fstatistic[2], df2 = summary(model_ns)$fstatistic[3], lower.tail = FALSE)
    r2_ns_sig10[i] = summary(model_ns)$r.squared
    
}
```


- **Results**

   - Empirical distribution for F stat
   
```{r}
par(mfrow = c(2,3))
# F stat for sigma = 1
#significant
hist(f_s_sig1,
     main   = "F stat-significant(sigma=1)",
     xlim = c(0, 80),
     ylim = c(0, 0.07),
     xlab   = "F stat",
     breaks = 12,
     col    = "dodgerblue",
     border = "darkorange",
     prob = TRUE
    )
x = f_s_sig1
curve( df(x, df1 = 4 - 1, df2 = 25 -4), col = "darkorange", add = TRUE, lwd = 3)

# F stat for sigma = 5
#significant
hist(f_s_sig5,
     main   = "F stat-significant(sigma=5)",
     xlim = c(0, 10),
     ylim = c(0, 0.6),
     xlab   = "F stat",
     breaks = 12,
     col    = "green",
     border = "darkorange",
     prob = TRUE
    )
x = f_s_sig5
curve( df(x, df1 = 4 - 1, df2 = 25 -4), col = "darkorange", add = TRUE, lwd = 3)

# F stat for sigma = 10
#significant
hist(f_s_sig10,
     main   = "F stat-significant(sigma=10)",
     xlim = c(0, 12),
     ylim = c(0, 0.6),
     xlab   = "F stat",
     breaks = 12,
     col    = "yellow",
     border = "darkorange",
     prob = TRUE
    )
x = f_s_sig10
curve( df(x, df1 = 4 - 1, df2 = 25 -4), col = "darkorange", add = TRUE, lwd = 3)



# F stat for sigma = 1
#non-significant
hist(f_ns_sig1,
     xlab   = "F stat",
     xlim = c(0, 10),
     ylim = c(0, 0.7),
     main   = "F stat-nonsignificant(sigma=1)",
     breaks = 12,
     col    = "dodgerblue",
     border = "darkorange",
     prob = TRUE)
x = f_ns_sig1
curve( df(x, df1 = 4 - 1, df2 = 25 - 4), col = "darkorange", add = TRUE, lwd = 3)

# F stat for sigma = 5
#non-significant
hist(f_ns_sig5,
     xlab   = "F stat",
     xlim = c(0, 10),
     ylim = c(0, 0.7),
     main   = "F stat-nonsignificant(sigma=5)",
     breaks = 12,
     col    = "green",
     border = "darkorange",
     prob = TRUE)
x = f_ns_sig5
curve( df(x, df1 = 4 - 1, df2 = 25 - 4), col = "darkorange", add = TRUE, lwd = 3)

# F stat for sigma = 10
#non-significant
hist(f_ns_sig10,
     xlab   = "F stat",
     xlim = c(0, 10),
     ylim = c(0, 0.7),
     main   = "F stat-nonsignificant(sigma=10)",
     breaks = 12,
     col    = "yellow",
     border = "darkorange",
     prob = TRUE)
x = f_ns_sig10
curve( df(x, df1 = 4 - 1, df2 = 25 - 4), col = "darkorange", add = TRUE, lwd = 3)
```
   
   
   

   - Empirical distribution for p-value
   
```{r}
# p-value for sigma = 1
par(mfrow = c(2, 3))
#significant
hist(p_val_s_sig1, 
     main   = "P-value-significant(sigma=1)",
     xlab   = "P value",
     breaks = 12,
     col    = "dodgerblue",
     border = "darkorange",
     prob = TRUE)

# p-value for sigma = 5
#significant
hist(p_val_s_sig5, 
     main   = "P-value-significant(sigma=5)",
     xlab   = "P value",
     breaks = 12,
     col    = "green",
     border = "darkorange",
     prob = TRUE)

# p-value for sigma = 10
#significant
hist(p_val_s_sig10, 
     main   = "P-value-significant(sigma=10)",
     xlab   = "P value",
     breaks = 12,
     col    = "yellow",
     border = "darkorange",
     prob = TRUE)

# p-value for sigma = 1
#non-significant
hist(p_val_ns_sig1, 
     main   = "P-value-non-significant(sigma=1)",
     xlab   = "P value",
     breaks = 12,
     col    = "dodgerblue",
     border = "darkorange",
     prob = TRUE)

# p-value for sigma = 5
#non-significant
hist(p_val_ns_sig5, 
     main   = "P-value-non-significant(sigma=5)",
     xlab   = "P value",
     breaks = 12,
     col    = "green",
     border = "darkorange",
     prob = TRUE)

# p-value for sigma = 10
#non-significant
hist(p_val_ns_sig10, 
     main   = "P-value-non-significant(sigma=10)",
     xlab   = "P value",
     breaks = 12,
     col    = "yellow",
     border = "darkorange",
     prob = TRUE)

```
   

   - Empirical distribution for $R ^ 2$
  
   $R ^ 2$ follows beta distribution, and the degrees of freedom are $(k-1)/2$ and $(n-k)/2$.


```{r}
par(mfrow = c(2, 3))
# R^2 (beta distribution) for sigma = 1
k = 4
n = 25
x = seq(0,1, by = 0.01)
#significant
hist(r2_s_sig1, 
     main   = "R^2-significant(sigma=1)",
     xlim = c(0, 1),
     xlab   = "R^2",
     breaks = 12,
     col    = "dodgerblue",
     border = "darkorange",
     probability = TRUE)
x = r2_s_sig1
curve(dbeta(x, shape1 = (k-1)/2, shape2 = (n-k)/2), add=TRUE, lwd = 3,col = "darkorange")


# R^2 (beta distribution) for sigma = 5
#significant
x = seq(0,1, by = 0.01)
hist(r2_s_sig5, 
     main   = "R^2-significant(sigma=5)",
     xlim = c(0, 1),
     ylim = c(0, 5),
     xlab   = "R^2",
     breaks = 12,
     col    = "green",
     border = "darkorange",
     probability = TRUE)
x = r2_s_sig5
curve(dbeta(x, shape1 = (k-1)/2, shape2 = (n-k)/2), add=TRUE, lwd = 3,col = "darkorange")


# R^2 (beta distribution) for sigma = 10
x = seq(0,1, by = 0.01)
#significant
hist(r2_s_sig10, 
     main   = "R^2-significant(sigma=10)",
     xlim = c(0, 1),
     ylim = c(0, 6),
     xlab   = "R^2",
     breaks = 12,
     col    = "yellow",
     border = "darkorange",
     probability = TRUE)
x = r2_s_sig10
curve(dbeta(x, shape1 = (k-1)/2, shape2 = (n-k)/2), add=TRUE, lwd = 3,col = "darkorange")



# R^2 (beta distribution) for sigma = 1
x = seq(0,1, by = 0.01)
#non-significant
hist(r2_ns_sig1, 
     main   = "R^2-non-significant(sigma=1)",
     xlab   = "R^2",
     ylim = c(0, 6),
     breaks = 12,
     col    = "dodgerblue",
     border = "darkorange",
     probability = TRUE)
x = r2_ns_sig1
curve(dbeta(x, shape1 = (k-1)/2, shape2 = (n-k)/2), add=TRUE, lwd = 3, col = "darkorange")

# R^2 (beta distribution) for sigma = 5
#non-significant
x = seq(0,1, by = 0.01)
hist(r2_ns_sig5, 
     main   = "R^2-non-significant(sigma=5)",
     xlab   = "R^2",
     ylim = c(0, 6),
     breaks = 12,
     col    = "green",
     border = "darkorange",
     probability = TRUE)
x = r2_ns_sig5
curve(dbeta(x, shape1 = (k-1)/2, shape2 = (n-k)/2), add=TRUE, lwd = 3, col = "darkorange")

# R^2 (beta distribution) for sigma = 10
x = seq(0,1, by = 0.01)
#non-significant
hist(r2_ns_sig10, 
     main   = "R^2-non-significant(sigma=10)",
     xlab   = "R^2",
     ylim = c(0,6),
     breaks = 12,
     col    = "yellow",
     border = "darkorange",
     probability = TRUE)
x = r2_ns_sig10
curve(dbeta(x, shape1 = (k-1)/2, shape2 = (n-k)/2), add=TRUE, lwd = 3, col = "darkorange")

```

 
 
- **Discussion**
   
   - The true distribution of $F$ statistic follows the standard F distribution. The above graphs show the empirical distribution of the $F$ statistics against the true standard F distribution. As shown, the $F$ statistics from significant models do not quite follow the distribution curve compared with the performance of the $F$ statistics from non-significant models. When $\sigma = 1$, the empirical distribution of $F$ statistics least aligns with the true F distribution. As the $\sigma$ increases, the empirical distribution of $F$ more align with the true F distribution.
   
   - The true distribution of p-value follows the uniform distribution, which should be a horizontal line in the graph with y-axis indicating density and x-axis indicating p-value. As shown in the graphs above, the empirical distribution of P-value from significant models less align with the true uniform distribution compared with the empirical distribution of p-value from non-significant models. Especially when $\sigma = 1$ for the significant model, there is a single bar on the left, which means the density is not evenly distributed. As $\sigma$ increases, p-value tend to increase, and empirical distribution becomes more uniform.
   
   - The true distribution of $R ^ 2$ follows the beta distribution, which is shown as a right-skewed curve. From the graph that I plotted, the empirical distribution of $R ^ 2$ from significant models less aligned with the true beta distribution curve compared with empirical distribution of $R ^ 2$ from non-significant models. As $\sigma$ increases, the empirical distribution of $R ^ 2$ from the significant models gets more aglined with the true beta distribution.
   
   
   - To answer the third question, again we look at the 6 $R ^ 2$ empirical distribution graphs. From the empirical distributions from significant models, we found that significant models with lower $\sigma$ often has higher $R ^ 2$ values on average. It makes sense since lower $\sigma$ means less noise, and it is less likely the variations will caused by noise but more likely that the variations will be explained by the model. As $\sigma$ increases, the empirical distribution starts to shift to the left with its mean closer to 0 and become more aligned with the beta distribution curve.
   
   - However, the empirical distribution of $R ^ 2$ is close to the true beta distribution, and it remains relatively stable and does not quite change with the $\sigma$. Therefore, the relationship between the $R ^ 2$ and $\sigma$ for the significant models might not be the same for the non-significant models.
 
 
 
 
 
 
 

## Simulation Study 2: Using RMSE for Selection?


- **Introduction**

  In this simulation study, we are going to use Train and Test RMSE to select the "best" model and investigate how effective this method is.
 
  We will be using the following model:
  \[
  Y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3} + \beta_3x_{i3} + \beta_4x_{i4} + \beta_5x_{i5} + \beta_6x_{i6} + \epsilon_i
\]

  Where $\epsilon_i \sim N(0, \sigma ^ 2).$ and
  
  - $\beta_0 = 0$
  - $\beta_1 = 5$
  - $\beta_2 = -4$
  - $\beta_3 = 1.6$
  - $\beta_4 = -1.1$
  - $\beta_5 = 0.7$
  - $\beta_6 = 0.3$
 
  We will consider a sample size of 500, and three possible levels of noise. That is three values of $\sigma$.

  - $n = 500$
  - $\sigma \in (1, 2, 4)$
 
  We will use the data found in [`study_2.csv`](study_2.csv) for the values of the predictors. These data are kept constant for the entirety of this study. The `y` values in this data are a blank placeholder.
  

  Each time we simulate the data, we will randomly split data into train and test sets with 250 observations for each, and we will fit the sets into each of the following models:

  - y ~ x1
  - y ~ x1 + x2
  - y ~ x1 + x2 + x3
  - y ~ x1 + x2 + x3 + x4
  - y ~ x1 + x2 + x3 + x4 + x5
  - y ~ x1 + x2 + x3 + x4 + x5 + x6, the correct form of the model
  - y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7
  - y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8
  - y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 
 
  For each model, We will be using the RMSE formula as below:

  \[
        \text{RMSE}(\text{model, data}) = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}
    \]
  
  We will repeat the simulation process for 1000 times for each of the 3 values of $\sigma$

  For each value of $\sigma$, we will create a plot that shows how average Train RMSE and average Test RMSE changes as a function of model size and shows the number of times the model of each size was chosen for each value of $\sigma$.

  Here are some questions that we need to consider during the simulation process:

    1. Does the method always select the correct model? On average, does is select the correct model?
    2. How does the level of noise affect the results?



- **Methods**

  We will start with setting the seeds and importing the data from [`study_2.csv`](study_2.csv) file. Then, we are going to fix the parameters before we start the simulation.
 

```{r}
#setting seeds
birthday = 19980121
set.seed(birthday)

#Importing data
library(readr)
study_2 = read.csv("study_2.csv")

#Fixing parameters
beta_0 = 0
beta_1 = 5
beta_2 = -4
beta_3 = 1.6
beta_4 = -1.1
beta_5 = 0.7
beta_6 = 0.3

n = 500
sigma = c(1, 2, 4)
sim_num = 1000

x1 = study_2$x1
x2 = study_2$x2
x3 = study_2$x3
x4 = study_2$x4
x5 = study_2$x5
x6 = study_2$x6
x7 = study_2$x7
x8 = study_2$x8
x9 = study_2$x9

#Creating result containers for simulation
mod1_train = rep(0, sim_num)
mod2_train = rep(0, sim_num)
mod3_train = rep(0, sim_num)
mod4_train = rep(0, sim_num)
mod5_train = rep(0, sim_num)
mod6_train = rep(0, sim_num)
mod7_train = rep(0, sim_num)
mod8_train = rep(0, sim_num)
mod9_train = rep(0, sim_num)

mod1_test = rep(0, sim_num)
mod2_test = rep(0, sim_num)
mod3_test = rep(0, sim_num)
mod4_test = rep(0, sim_num)
mod5_test = rep(0, sim_num)
mod6_test = rep(0, sim_num)
mod7_test = rep(0, sim_num)
mod8_test = rep(0, sim_num)
mod9_test = rep(0, sim_num)

#RMSE function
rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}
```


   Then, we are going to start simulations for each $\sigma$ value separately. We will start with $\sigma = 1$.
   
   
```{r}
#Simulation with sigma = 1
 for(i in 1:sim_num){
    eps = rnorm(n, mean = 0, sd = 1)
    study_2$y = beta_0  + beta_1 * x1 + beta_2 * x2 + beta_3 * x3 + beta_4 * x4 + beta_5 * x5 + beta_6 * x6 + eps
    
    #Split the data into train and test sets with 250 observations each.
    train_idx = sample(1:nrow(study_2), 250) # randomly select observations for training
    train = study_2[train_idx, ]
    test = study_2[-train_idx, ]
    
    #Fit the nine models
    mod_1 = lm(y ~ x1, data = train)
    mod_2 = lm(y ~ x1 + x2, data = train)
    mod_3 = lm(y ~ x1 + x2 + x3, data = train)
    mod_4 = lm(y ~ x1 + x2 + x3 + x4, data = train)
    mod_5 = lm(y ~ x1 + x2 + x3 + x4 + x5, data = train)
    mod_6 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6, data = train)
    mod_7 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7, data = train)
    mod_8 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = train)
    mod_9 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, data = train)
    
    mod1_train[i] = rmse(train$y, predict(mod_1, train))
    mod2_train[i] = rmse(train$y, predict(mod_2, train))
    mod3_train[i] = rmse(train$y, predict(mod_3, train))
    mod4_train[i] = rmse(train$y, predict(mod_4, train))
    mod5_train[i] = rmse(train$y, predict(mod_5, train))
    mod6_train[i] = rmse(train$y, predict(mod_6, train))
    mod7_train[i] = rmse(train$y, predict(mod_7, train))
    mod8_train[i] = rmse(train$y, predict(mod_8, train))
    mod9_train[i] = rmse(train$y, predict(mod_9, train))
    
    mod1_test[i] = rmse(test$y, predict(mod_1, test))
    mod2_test[i] = rmse(test$y, predict(mod_2, test))
    mod3_test[i] = rmse(test$y, predict(mod_3, test))
    mod4_test[i] = rmse(test$y, predict(mod_4, test))
    mod5_test[i] = rmse(test$y, predict(mod_5, test))
    mod6_test[i] = rmse(test$y, predict(mod_6, test))
    mod7_test[i] = rmse(test$y, predict(mod_7, test))
    mod8_test[i] = rmse(test$y, predict(mod_8, test))
    mod9_test[i] = rmse(test$y, predict(mod_9, test))
 }

    #Store the results
    train_rmse_sig1 = data.frame(mod1_train, mod2_train, mod3_train, mod4_train, mod5_train, mod6_train, mod7_train, mod8_train, mod9_train)
    test_rmse_sig1 = data.frame(mod1_test, mod2_test, mod3_test, mod4_test, mod5_test, mod6_test, mod7_test, mod8_test, mod9_test)
```

 
   Then, we will run the simulation for $\sigma = 2$.
   
   
```{r}
#Simulation with sigma = 2
 for(i in 1:sim_num){
    eps = rnorm(n, mean = 0, sd = 1)
    study_2$y = beta_0 + beta_1 * x1 + beta_2 * x2 + beta_3 * x3 + beta_4 * x4 + beta_5 * x5 + beta_6 * x6 + eps
    
    #Split the data into train and test sets with 250 observations each.
    train_idx = sample(1:nrow(study_2), 250) # randomly select observations for training
    train = study_2[train_idx, ]
    test = study_2[-train_idx, ]
    
    #Fit the nine models
    mod_1 = lm(y ~ x1, data = train)
    mod_2 = lm(y ~ x1 + x2, data = train)
    mod_3 = lm(y ~ x1 + x2 + x3, data = train)
    mod_4 = lm(y ~ x1 + x2 + x3 + x4, data = train)
    mod_5 = lm(y ~ x1 + x2 + x3 + x4 + x5, data = train)
    mod_6 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6, data = train)
    mod_7 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7, data = train)
    mod_8 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = train)
    mod_9 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, data = train)
    
    mod1_train[i] = rmse(train$y, predict(mod_1, train))
    mod2_train[i] = rmse(train$y, predict(mod_2, train))
    mod3_train[i] = rmse(train$y, predict(mod_3, train))
    mod4_train[i] = rmse(train$y, predict(mod_4, train))
    mod5_train[i] = rmse(train$y, predict(mod_5, train))
    mod6_train[i] = rmse(train$y, predict(mod_6, train))
    mod7_train[i] = rmse(train$y, predict(mod_7, train))
    mod8_train[i] = rmse(train$y, predict(mod_8, train))
    mod9_train[i] = rmse(train$y, predict(mod_9, train))
    
    mod1_test[i] = rmse(test$y, predict(mod_1, test))
    mod2_test[i] = rmse(test$y, predict(mod_2, test))
    mod3_test[i] = rmse(test$y, predict(mod_3, test))
    mod4_test[i] = rmse(test$y, predict(mod_4, test))
    mod5_test[i] = rmse(test$y, predict(mod_5, test))
    mod6_test[i] = rmse(test$y, predict(mod_6, test))
    mod7_test[i] = rmse(test$y, predict(mod_7, test))
    mod8_test[i] = rmse(test$y, predict(mod_8, test))
    mod9_test[i] = rmse(test$y, predict(mod_9, test))
 }

    #Store the results
    train_rmse_sig2 = data.frame(mod1_train, mod2_train, mod3_train, mod4_train, mod5_train, mod6_train, mod7_train, mod8_train, mod9_train)
    test_rmse_sig2 = data.frame(mod1_test, mod2_test, mod3_test, mod4_test, mod5_test, mod6_test, mod7_test, mod8_test, mod9_test)
```
   
   
  Finally, we will run the simulation for $\sigma = 4$.
  
```{r}
#Simulation with sigma = 4
 for(i in 1:sim_num){
    eps = rnorm(n, mean = 0, sd = 1)
    study_2$y = beta_0 + beta_1 * x1 + beta_2 * x2 + beta_3 * x3 + beta_4 * x4 + beta_5 * x5 + beta_6 * x6 + eps
    
    #Split the data into train and test sets with 250 observations each.
    train_idx = sample(1:nrow(study_2), 250) # randomly select observations for training
    train = study_2[train_idx, ]
    test = study_2[-train_idx, ]
    
    #Fit the nine models
    mod_1 = lm(y ~ x1, data = train)
    mod_2 = lm(y ~ x1 + x2, data = train)
    mod_3 = lm(y ~ x1 + x2 + x3, data = train)
    mod_4 = lm(y ~ x1 + x2 + x3 + x4, data = train)
    mod_5 = lm(y ~ x1 + x2 + x3 + x4 + x5, data = train)
    mod_6 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6, data = train)
    mod_7 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7, data = train)
    mod_8 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = train)
    mod_9 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, data = train)
    
    mod1_train[i] = rmse(train$y, predict(mod_1, train))
    mod2_train[i] = rmse(train$y, predict(mod_2, train))
    mod3_train[i] = rmse(train$y, predict(mod_3, train))
    mod4_train[i] = rmse(train$y, predict(mod_4, train))
    mod5_train[i] = rmse(train$y, predict(mod_5, train))
    mod6_train[i] = rmse(train$y, predict(mod_6, train))
    mod7_train[i] = rmse(train$y, predict(mod_7, train))
    mod8_train[i] = rmse(train$y, predict(mod_8, train))
    mod9_train[i] = rmse(train$y, predict(mod_9, train))
    
    mod1_test[i] = rmse(test$y, predict(mod_1, test))
    mod2_test[i] = rmse(test$y, predict(mod_2, test))
    mod3_test[i] = rmse(test$y, predict(mod_3, test))
    mod4_test[i] = rmse(test$y, predict(mod_4, test))
    mod5_test[i] = rmse(test$y, predict(mod_5, test))
    mod6_test[i] = rmse(test$y, predict(mod_6, test))
    mod7_test[i] = rmse(test$y, predict(mod_7, test))
    mod8_test[i] = rmse(test$y, predict(mod_8, test))
    mod9_test[i] = rmse(test$y, predict(mod_9, test))
 }

    #Store the results
    train_rmse_sig4 = data.frame(mod1_train, mod2_train, mod3_train, mod4_train, mod5_train, mod6_train, mod7_train, mod8_train, mod9_train)
    test_rmse_sig4 = data.frame(mod1_test, mod2_test, mod3_test, mod4_test, mod5_test, mod6_test, mod7_test, mod8_test, mod9_test)
```
  
   
   
   
  

- **Results**

  After we get the 6 data frames generated from the simulation, we are going to calculate the mean and plot it.
  
  
```{r}
par(mfrow = c(3,2))
#sigma = 1
mean_train = apply(train_rmse_sig1, 2, mean)
mean_test = apply(test_rmse_sig1, 2, mean)
rmse_train_mean = data.frame(model = names(mean_train), mean_train)
rmse_test_mean = data.frame(model = names(mean_test), mean_test)


plot(mean_train ~ model, 
     data = rmse_train_mean,
     main="Average Train RMSE vs. Model Size (sigma=1)", 
     xlab="Model Size", 
     ylab="Average Train RMSE", 
     pch=18, 
     col="dodgerblue",
     border = "darkorange", 
     ylim = c(0,3.5), 
     cex.axis = 0.5) + text(rmse_train_mean$model,                rmse_train_mean$mean_train, 
labels = round(rmse_train_mean$mean_train,4),pos=3,cex=0.8, col="red")


#sigma = 1
plot(mean_test ~ model, 
     data = rmse_test_mean,
     main="Average Test RMSE vs. Model Size(sigma=1)", 
     xlab="Model Size", 
     ylab="Average Test RMSE", 
     pch=18, 
     col="dodgerblue",
     border = "darkorange", 
     ylim = c(0,3.5), 
     cex.axis = 0.5) + 
  text(rmse_test_mean$model, rmse_test_mean$mean_test, labels = round(rmse_test_mean$mean_test,4),pos = 3,cex = 0.8, col="red")





#Sigma = 2
mean_train = apply(train_rmse_sig2, 2, mean)
mean_test = apply(test_rmse_sig2, 2, mean)
rmse_train_mean = data.frame(model = names(mean_train), mean_train)
rmse_test_mean = data.frame(model = names(mean_test), mean_test)


plot(mean_train ~ model, 
     data = rmse_train_mean,
     main="Average Train RMSE vs. Model Size(sigma=2)", 
     xlab="Model Size", 
     ylab="Average Train RMSE", 
     pch=18, 
     col="dodgerblue",
     border = "darkorange", 
     ylim = c(0,3.5), 
     cex.axis = 0.5) + text(rmse_train_mean$model,rmse_train_mean$mean_train, labels = round(rmse_train_mean$mean_train,4),pos=3,cex=0.8, col="red")


plot(mean_test ~ model, 
     data = rmse_test_mean,
     main="Average Test RMSE vs. Model Size(sigma=2)", 
     xlab="Model Size", 
     ylab="Average Test RMSE", 
     pch=18, 
     col="dodgerblue",
     border = "darkorange", 
     ylim = c(0,3.5), 
     cex.axis = 0.5) + text(rmse_test_mean$model, rmse_test_mean$mean_test, labels = round(rmse_test_mean$mean_test,4),pos=3,cex=0.8, col="red")

#Sigma = 4
mean_train = apply(train_rmse_sig4, 2, mean)
mean_test = apply(test_rmse_sig4, 2, mean)
rmse_train_mean = data.frame(model = names(mean_train), mean_train)
rmse_test_mean = data.frame(model = names(mean_test), mean_test)


plot(mean_train ~ model, 
     data = rmse_train_mean,
     main="Average Train RMSE vs. Model Size(sigma=4)", 
     xlab="Model Size", 
     ylab="Average Train RMSE", 
     pch=18, 
     col="dodgerblue",
     border = "darkorange", 
     ylim = c(0,3.5), 
     cex.axis = 0.5) + text(rmse_train_mean$model,rmse_train_mean$mean_train, labels = round(rmse_train_mean$mean_train,4),pos=3,cex=0.8, col="red")


plot(mean_test ~ model, 
     data = rmse_test_mean,
     main="Average Test RMSE vs. Model Size(sigma=4)", 
     xlab="Model Size", ylab="Average Test RMSE", 
     pch=18, 
     col="dodgerblue",
     border = "darkorange", 
     ylim = c(0,3.5), 
     cex.axis = 0.5) + text(rmse_test_mean$model, rmse_test_mean$mean_test, labels = round(rmse_test_mean$mean_test,4),pos=3,cex=0.8, col="red")
```
  
   
   
   Next, we'll plot graphs that show the number of times the model of each size was chosen for each value of $\sigma$.
   
   
```{r}
par(mfrow = c(1,3))
#Sigma = 1
min_test_row_sig1 = apply(test_rmse_sig1, 1, which.min)
df_sig1 = as.data.frame(table(min_test_row_sig1))
          
plot(Freq ~ min_test_row_sig1, data = as.data.frame(table(min_test_row_sig1)), 
     main = "Model size of selection frequency (sigma = 1)",
     cex.main=0.6,
     cex.axis=1, 
     cex.lab= 1, 
     xlab="Model Size", 
     ylim = c(0, 600)) + text(as.data.frame(table(min_test_row_sig1))$min_test_row_sig1, as.data.frame(table(min_test_row_sig1))$Freq, labels = round(as.data.frame(table(min_test_row_sig1))$Freq, 0),pos=3,cex=0.8, col="red")



#Sigma = 2
min_test_row_sig2 = apply(test_rmse_sig2, 1, which.min)
df_sig2 = as.data.frame(table(min_test_row_sig2))
          

plot(Freq ~ min_test_row_sig2, 
     data = as.data.frame(table(min_test_row_sig2)), 
     main = "Model size of selection frequency (sigma = 2)",
     cex.main= 0.6,
     cex.axis=1, 
     cex.lab= 1, 
     xlab="Model Size", 
     ylim = c(0, 650)) + text(as.data.frame(table(min_test_row_sig2))$min_test_row_sig2, as.data.frame(table(min_test_row_sig2))$Freq, labels = round(as.data.frame(table(min_test_row_sig2))$Freq, 0),pos=3,cex=0.8, col="red")

#Sigma = 4
min_test_row_sig4 = apply(test_rmse_sig4, 1, which.min)
df_sig4 = as.data.frame(table(min_test_row_sig4))
          

plot(Freq ~ min_test_row_sig4, 
     data = as.data.frame(table(min_test_row_sig4)), 
     main = "Model size of selection frequency (sigma = 4)",
     cex.main = 0.6,
     cex.axis=1, 
     cex.lab= 1, 
     xlab="Model Size", 
     ylim = c(0, 650)) + text(as.data.frame(table(min_test_row_sig4))$min_test_row_sig4, as.data.frame(table(min_test_row_sig4))$Freq, labels = round(as.data.frame(table(min_test_row_sig4))$Freq, 0),pos=3,cex=0.8, col="red")
```
   
   
   
   
- **Discussion**

  - To answer the first question, we first look at the "Model size selection frequency" graphs. The three graphs show the number of times that specific model has the lowest RMSE and is considered as the best model among the nine models under three levels of noise. Even though model 6 has the highest probability of being selected, the model which has the lowest Test RMSE is not unique, **so we conclude that the method does not always select the correct model**. 
  - However, **on average, the method select the best model**. As can see from the 3 "Average Test RMSE vs. Model Size" graphs, model 6 always has the lowest Test RMSE value, and model 6 is the correct form of the model as stated in the introduction. Therefore, on average, the method selects the correct model.
  - To answer the second question, we first look at the three "model size selection frequency chart", finding that as level of noise increases(i.e. $\sigma$ increases), the times that the models being selected vary, but model 6 always has the highest probability of being the best model under all three levels of noise. Then, we look at the "Average Test RMSE vs. Model Size" graphs, finding that as level of noise increases, model 6 always has the lowest average Test RMSE, and its average Test RMSE is slightly decreasing.