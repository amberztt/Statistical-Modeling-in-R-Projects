---
title: "Week 8 - Homework"
author: "STAT 420, Summer 2018, Unger"
date: ''
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.alin = "center")
```

## Exercise 1 (Writing Functions)

**(a)** Write a function named `diagnostics` that takes as input the arguments:

- `model`, an object of class `lm()`, that is a model fit via `lm()`
- `pcol`, for controlling point colors in plots, with a default value of `grey`
- `lcol`, for controlling line colors in plots, with a default value of `dodgerblue`
- `alpha`, the significance level of any test that will be performed inside the function, with a default value of `0.05`
- `plotit`, a logical value for controlling display of plots with default value `TRUE`
- `testit`, a logical value for controlling outputting the results of tests with default value `TRUE`

The function should output:

- A list with two elements when `testit` is `TRUE`:
    - `p_val`, the p-value for the Shapiro-Wilk test for assessing normality
    - `decision`, the decision made when performing the Shapiro-Wilk test using the `alpha` value input to the function. "Reject" if the null hypothesis is rejected, otherwise "Fail to Reject."
- Two plots, side-by-side, when `plotit` is `TRUE`:
    - A fitted versus residuals plot that adds a horizontal line at $y = 0$, and labels the $x$-axis "Fitted" and the $y$-axis "Residuals." The points and line should be colored according to the input arguments. Give the plot a title. 
    - A Normal Q-Q plot of the residuals that adds the appropriate line using `qqline()`. The points and line should be colored according to the input arguments. Be sure the plot has a title. 

Consider using this function to help with the remainder of the assignment as well.


```{r}
diagnostics = function(model, pcol = "grey", lcol = "dodgerblue", alpha = 0.05, plotit = TRUE, testit = TRUE){
  if(plotit == TRUE){
    par(mfrow = c(1,2))
    plot(fitted(model), resid(model), col = pcol, pch = 20, xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals")
    abline(h = 0, col = lcol, lwd = 2)
    
    qqnorm(resid(model), main = "Normal Q-Q plot",  col = pcol)
    qqline(resid(model), col = lcol, lwd = 2)
  }
  
  if(testit == TRUE){
    p_val = shapiro.test(resid(model))$p.value
    decision = ifelse(alpha > p_val, "Reject", "Fail to reject")
    list(p_val = p_val, decision = decision)
  }
}
  
```


**(b)** Run the following code.

```{r}
set.seed(420)

data_1 = data.frame(x = runif(n = 30, min = 0, max = 10),
                    y = rep(x = 0, times = 30))
data_1$y = with(data_1, 2 + 1 * x + rexp(n = 30))
fit_1 = lm(y ~ x, data = data_1)

data_2 = data.frame(x = runif(n = 20, min = 0, max = 10),
                    y = rep(x = 0, times = 20))
data_2$y = with(data_2, 5 + 2 * x + rnorm(n = 20))
fit_2 = lm(y ~ x, data = data_2)

data_3 = data.frame(x = runif(n = 40, min = 0, max = 10),
                    y = rep(x = 0, times = 40))
data_3$y = with(data_3, 2 + 1 * x + rnorm(n = 40, sd = x))
fit_3 = lm(y ~ x, data = data_3)
```

```{r}
diagnostics(fit_1, plotit = FALSE)$p_val
diagnostics(fit_2, plotit = FALSE)$decision
diagnostics(fit_1, testit = FALSE, pcol = "black", lcol = "black")
diagnostics(fit_2, testit = FALSE, pcol = "grey", lcol = "green")
diagnostics(fit_3)
```

***

## Exercise 2 (Prostate Cancer Data)

For this exercise, we will use the `prostate` data, which can be found in the `faraway` package. After loading the `faraway` package, use `?prostate` to learn about this dataset.

```{r, message = FALSE, warning = FALSE}
library(faraway)
```

**(a)** Fit an additive multiple regression model with `lpsa` as the response and the remaining variables in the `prostate` dataset as predictors. Report the $R^2$ value for this model.

```{r}
prostate_model = lm(lpsa ~ ., data = prostate)
summary(prostate_model)$r.squared
```



**(b)** Check the constant variance assumption for this model. Do you feel it has been violated? Justify your answer.


```{r, message = FALSE, warning = FALSE}
plot(fitted(prostate_model), resid(prostate_model), col = "gray", pch = 20, xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals")

library(lmtest)
bptest(prostate_model)
```


 - The fitted versus residuals plot seems good, and there seem to be a constant variance.
 - From the Breusch-Pagan test, we get the p-value which is 0.3. Therefore, we do not reject the null hypothesis, and we conclude the constant variance assumption has not been violated.
 

**(c)** Check the normality assumption for this model. Do you feel it has been violated? Justify your answer.

```{r}
#Q-Q plot
qqnorm(resid(prostate_model), main = "Normal Q-Q plot, prostate model", col = "gray")
qqline(resid(prostate_model), col = "dodgerblue", lwd = 2 )

#Shapiro-wilk test
shapiro.test(resid(prostate_model))
```



 - From the Q-Q plot above, it seems the points are closely aligned with the line. The plot indicates the normal distribution of the data.
 - We performed the Shapiro-Wilk normality test and get the p-value of 0.8, so we do not reject the null hypothesis. Therefore, the normality assumption for this model has not been violated.
 

**(d)** Check for any high leverage observations. Report any observations you determine to have high leverage.

```{r}
hats = hatvalues(prostate_model) > 2 * mean(hatvalues(prostate_model))
prostate[hats,]

```


 - The observations shown above(NO.32, 37, 41, 74, 92) are the ones which are determined to have high leverages.


**(e)** Check for any influential observations. Report any observations you determine to be influential.

```{r}
cd = cooks.distance(prostate_model) > 4 / length(cooks.distance(prostate_model))
prostate[cd,]
```


 - The 7 observations shown above(NO.32, 39,47,69,95,96,97) are considered to be influential.


**(f)** Refit the additive multiple regression model without any points you identified as influential. Compare the coefficients of this fitted model to the previously fitted model.

```{r}
prostate_remove = prostate[-which(cd == TRUE),]
prostate_re_model = lm(lpsa ~ ., data = prostate_remove)

#Coefficients of original model
coef(prostate_model)

#Coefficients of the model without influential points
coef(prostate_re_model)
```


 - Comparing the coefficients of the original model and the coefficients of the model without influential points, we found that there is a relatively large negative change on Intercepts. Also, coefficients of lweight and gleason have relatively large changes while other parameters have very slight changes.
 

**(g)** Create a data frame that stores the observations that were "removed" because they were influential. Use the two models you have fit to make predictions with these observations. Comment on the difference between these two sets of predictions.

```{r}
influential_obs = prostate[cd, ]
influential_obs = as.data.frame(influential_obs)

#Prediction using original model
predict(prostate_model, newdata = influential_obs)
#Prediction using removed model
predict(prostate_re_model, newdata = influential_obs)
```

 - The two sets of the predictions differ with each other. Some predictions(39, 95, 97) are larger in the original model while some predicitons(32, 47, 69, 96) are larger in the removed model. The reason why they differs might be the influence of the removed observations they have on the two models are different. For the original model, since removed observations are contained in the data sets, the predictions are more approaching to the actual value they have. However, the removed model does not have the removed observations, so the predictions might be more approaching to the general trend of the data points which are not considered as influential.


***

## Exercise 3 (Why Bother?)

**Why** do we care about violations of assumptions? One key reason is that the distributions of the parameter esimators that we have used are all reliant on these assumptions. When the assumptions are violated, the distributional results are not correct, so our tests are garbage. **Garbage In, Garbage Out!**

Consider the following setup that we will use for the remainder of the exercise. We choose a sample size of 50.

```{r}
n = 50
set.seed(420)
x_1 = runif(n, 0, 5)
x_2 = runif(n, -2, 2)
```

Consider the model,

\[
Y = 4 + 1 x_1 + 0 x_2 + \epsilon.
\]

That is,

- $\beta_0$ = 4
- $\beta_1$ = 1
- $\beta_2$ = 0

We now simulate `y_1` in a manner that does **not** violate any assumptions, which we will verify. In this case $\epsilon \sim N(0, 1).$

```{r}
set.seed(1)
y_1 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n = n, mean = 0, sd = 1)
fit_1 = lm(y_1 ~ x_1 + x_2)
bptest(fit_1)
```

Then, we simulate `y_2` in a manner that **does** violate assumptions, which we again verify. In this case $\epsilon \sim N(0, \sigma = |x_2|).$

```{r}
set.seed(1)
y_2 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n = n, mean = 0, sd = abs(x_2))
fit_2 = lm(y_2 ~ x_1 + x_2)
bptest(fit_2)
```

**(a)** Use the following code after changing `birthday` to your birthday.

```{r}
num_sims = 2500
p_val_1 = rep(0, num_sims)
p_val_2 = rep(0, num_sims)
birthday = 19980121
set.seed(birthday)
```

Repeat the above process of generating `y_1` and `y_2` as defined above, and fit models with each as the response `2500` times. Each time, store the p-value for testing,

\[
\beta_2 = 0,
\]

using both models, in the appropriate variables defined above. (You do not need to use a data frame as we have in the past. Although, feel free to modify the code to instead use a data frame.)


```{r}
for(i in 1:num_sims){
  y_1 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n, 0, 1)
  fit_1 = lm(y_1 ~ x_1 + x_2)
  p_val_1[i] = summary(fit_1)$coefficients[3, 4]
  
  y_2 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n, 0, abs(x_2))
  fit_2 = lm(y_2 ~ x_1 + x_2)
  p_val_2[i] = summary(fit_2)$coefficients[3, 4]
}
```



**(b)** What proportion of the `p_val_1` values is less than 0.01? Less than 0.05? Less than 0.10? What proportion of the `p_val_2` values is less than 0.01? Less than 0.05? Less than 0.10? Arrange your results in a table. Briefly explain these results.

```{r}
library(knitr)

pval_results = data.frame(
  "y_1" = c(
    "< 0.01" = mean(p_val_1 < 0.01),
    "< 0.05" = mean(p_val_1 < 0.05),
    "< 0.10" = mean(p_val_1 < 0.10)
  ),
  "y_2" = c(
    "< 0.01" = mean(p_val_2 < 0.01),
    "< 0.05" = mean(p_val_2 < 0.05),
    "< 0.10" = mean(p_val_2 < 0.10)
  )
)
kable(pval_results, caption = "p-value proportions")
```


 - For the model that does not violate any assumptions, we expect the proportion of the p-values being less than the given $\alpha$ to be approximately equal to the given $\alpha$.
 - For the model that violates the assumptions, the proportion of the p-values being less than the given $\alpha$ is higher than the given $\alpha$. That is to say, for given $\alpha$, we are more often to reject the null hypothesis compared to the first model which does not violate any assumptions.

***

## Exercise 4 (Corrosion Data)

For this exercise, we will use the `corrosion` data, which can be found in the `faraway` package. After loading the `faraway` package, use `?corrosion` to learn about this dataset.

```{r, message = FALSE, warning = FALSE}
library(faraway)
```

**(a)** Fit a simple linear regression with `loss` as the response and `Fe` as the predictor. Plot a scatterplot and add the fitted line. Check the assumptions of this model.

```{r}
corrosion_slr = lm(loss ~ Fe, data = corrosion)
plot(loss ~ Fe, data = corrosion, col = "dodgerblue", pch = 20, cex = 1.5, main = "Weight loss per day vs Fe", xlab = "Fe", ylab = "loss")
abline(corrosion_slr, col = "orange", lwd = 2)

par(mfrow = c(1, 2))

#Fitted vs Residuals
plot(fitted(corrosion_slr), resid(corrosion_slr), col = "gray", pch = 20, xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals")
abline(h = 0, col = "dodgerblue", lwd = 2)

#Q-Q Plot
qqnorm(resid(corrosion_slr), main = "Normal Q-Q plot", col = "gray")
qqline(resid(corrosion_slr), col = "dodgerblue", lwd = 2)

#BP Test
bptest(corrosion_slr)

#shapiro-wilk test
shapiro.test(resid(corrosion_slr))
```



 - From the scatterplot, we found the linear relationship exists between loss and Fe.
 - The Fitted vs. Residuals plot looks ok, but its variance does not seem to be constant.
 - Looking at the BP test, we get the p-value which is 0.9, so we fail to reject the null hypothesis, indicating no suspect for constant variance.
 - The Q-Q plot looks ok but a bit weird, the points do not very closely align with the line.
 - We performed the Shapiro-wilk test and get the p-value which is 0.4, so we fail to reject the null hypothesis. Therefore, there is no suspect for normality assumption as well.



**(b)** Fit higher order polynomial models of degree 2, 3, and 4. For each, plot a fitted versus residuals plot and comment on the constant variance assumption. Based on those plots, which of these three models do you think are acceptable? Use a statistical test(s) to compare the models you just chose. Based on the test, which is preferred? Check the normality assumption of this model. Identify any influential observations of this model.

```{r}
corrosion_mod_2 = lm(loss ~ Fe + I(Fe ^ 2), data = corrosion)
corrosion_mod_3 = lm(loss ~ Fe + I(Fe ^ 2) + I(Fe ^ 3), data = corrosion)
corrosion_mod_4 = lm(loss ~ Fe + I(Fe ^ 2) + I(Fe ^ 3) + I(Fe ^ 4), data = corrosion)

par(mfrow = c(1, 3))

# Fitted vs Residuals degree = 2
plot(fitted(corrosion_mod_2), resid(corrosion_mod_2), col = "gray", pch = 20, xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals degree=2")
abline(h = 0, col = "dodgerblue", lwd = 2)

# Fitted vs Residuals degree = 3
plot(fitted(corrosion_mod_3), resid(corrosion_mod_3), col = "gray", pch = 20, xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals degree=3")
abline(h = 0, col = "dodgerblue", lwd = 2)

# Fitted vs Residuals degree = 4
plot(fitted(corrosion_mod_4), resid(corrosion_mod_4), col = "gray", pch = 20, xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals degree=4")
abline(h = 0, col = "dodgerblue", lwd = 2)

```


 - For the Fitted vs Residuals plot of the model with degree = 2, it fits better than the original plot we have, but it still sort of violating the constant variance assumptions.
 - Moving on to the plot of the model with degree = 3, it looks better than the plot of the model with degree = 2.
 - Then, looking at the plot of the model with degree = 4, it appears to somewhat violating the constant variance assumption compared with the model with degree equals to 3.
 - Based on the plot, the plot of the model with degree = 3 seems acceptable, and the plot of the model with degree = 4 is less acceptable than the plot of the model with degree = 3, but still considered somewhat acceptable. Since there is just a few observations, it is hard to truly determine if the models violate the constant variance assumptions or not.
 
 
```{r}
#compare the models
anova(corrosion_mod_3, corrosion_mod_4)
```
 

 - We compared the model with degree = 3 and the model with degree = 4, and we got the p-value which is equal to 0.17, larger than $\alpha$ = 0.10 or 0.05. Therefore, we do not reject the null hypothesis and choose the model of degree 3.
 
 
```{r}
#check the normality assumption of the model with degree = 3

qqnorm(resid(corrosion_mod_3), main = "Normal Q-Q plot, degree 3", col = "gray")
qqline(resid(corrosion_mod_3), col = "dodgerblue", lwd = 2 )

#Shapiro-wilk test
shapiro.test(resid(corrosion_mod_3))
```
 
 
 - From the Q-Q plot above, it seems the points are closely aligned with the line. The plot indicates the normal distribution of the data.
 - Then, we performed the Shapiro-Wilk normality test and get the p-value of 0.9, so we do not reject the null hypothesis. Therefore, the normality assumption for this model has not been violated.
 
 
```{r}
#Influential observations
cook_3 = cooks.distance(corrosion_mod_3) > 4 / length(cooks.distance(corrosion_mod_3))
corrosion_mod_3[cook_3]

cooks.distance(corrosion_mod_3) > 4 / length(cooks.distance(corrosion_mod_3))
```


 - According to cook's distance test, there is no observations of model with degree 3 identifed as influential.

***

## Exercise 5 (Diamonds)

The data set `diamonds` from the `ggplot2` package contains prices and characteristics of 54,000 diamonds. For this exercise, use `price` as the response variable $y$, and `carat` as the predictor $x$. Use `?diamonds` to learn more.

```{r, message = FALSE, warning = FALSE}
library(ggplot2)
```

**(a)** Fit a linear model with `price` as the response variable $y$, and `carat` as the predictor $x$. Return the summary information of this model.

```{r}
diamonds_model = lm(price ~ carat, data = diamonds)
summary(diamonds_model)
```



**(b)** Plot a scatterplot of price versus carat and add the line for the fitted model in part **(a)**. Using a fitted versus residuals plot and/or a Q-Q plot, comment on the diagnostics. 


```{r}
plot(price ~ carat, data = diamonds, col = "dodgerblue", pch = 20, cex = 1.5, main = "Price vs carat", xlab = "carat", ylab = "price")
abline(diamonds_model, col = "orange", lwd = 2)

par(mfrow = c(1, 2))

# Fitted vs Residuals
plot(fitted(diamonds_model), resid(diamonds_model), col = "gray", pch = 20, xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals")
abline(h = 0, col = "dodgerblue", lwd = 2)

#Q-Q plot
qqnorm(resid(diamonds_model), main = "Normal Q-Q plot", col = "gray")
qqline(resid(diamonds_model), col = "dodgerblue", lwd = 2 )
```


 - From the above three plots, we found the violation of linearity, normality and constant variance. From the Fitted vs Residuals plot, the mean of the residuals does not average to 0, and there is a pattern of the decreasing variance, which violates the constant variance assumption. From the Q-Q plot, the tails at both ends do not align with the qqline, which violates the normality assumption.


**(c)** Seeing as the price stretches over several orders of magnitude, it seems reasonable to try a log transformation of the response. Fit a model with a logged response, plot a scatterplot of log-price versus carat and add the line for the fitted model, then use a fitted versus residuals plot and/or a Q-Q plot to comment on the diagnostics of the model.

```{r}
qplot(price, data = diamonds, bins = 30)
```


```{r}
diamonds_logres_model = lm(log(price) ~ carat, data = diamonds)
plot(log(price) ~ carat, data = diamonds, 
     col = "dodgerblue", 
     pch = 20, 
     cex = 1.5, 
     main = "log(Price) vs carat", 
     xlab = "carat", 
     ylab = "log(price)")
abline(diamonds_logres_model, col = "orange", lwd = 2)

par(mfrow = c(1, 2))

# Fitted vs Residuals
plot(fitted(diamonds_logres_model), resid(diamonds_logres_model), col = "gray", pch = 20, xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals")
abline(h = 0, col = "dodgerblue", lwd = 2)

#Q-Q plot
qqnorm(resid(diamonds_logres_model), main = "Normal Q-Q plot", col = "gray")
qqline(resid(diamonds_logres_model), col = "dodgerblue", lwd = 2 )


```


 - The three plots above are clearly better than the original ones presented in the **(b)**, but they are still violating linearity, constant variance, and normality assumptions. 
 - From the scatterplot, the points still look not linear but a bit log-like shape.
 - From the Fitted vs residuals plot, residuals still don't average to 0, and the decreasing pattern of variances exists, indicating the violation of constant variance assumption.
 - From the Q-Q plot, one tail has been fixed, but there is still one other tail deviating from the qqline, indicating the violation of normality assumption.



**(d)** Try adding log transformation of the predictor. Fit a model with a logged response and logged predictor, plot a scatterplot of log-price versus log-carat and add the line for the fitted model, then use a fitted versus residuals plot and/or a Q-Q plot to comment on the diagnostics of the model.

```{r}
diamonds_log_model = lm(log(price) ~ log(carat), data = diamonds)
plot(log(price) ~ log(carat), data = diamonds,      col = "dodgerblue", 
     pch = 20, 
     cex = 1.5, 
     main = "log(Price) vs log(carat)", 
     xlab = "log(carat)", 
     ylab = "log(price)")
abline(diamonds_log_model, col = "orange", lwd = 2)

par(mfrow = c(1, 2))

# Fitted vs Residuals
plot(fitted(diamonds_log_model), resid(diamonds_log_model), col = "gray", pch = 20, xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals")
abline(h = 0, col = "dodgerblue", lwd = 2)

#Q-Q plot
qqnorm(resid(diamonds_log_model), main = "Normal Q-Q plot", col = "gray")
qqline(resid(diamonds_log_model), col = "dodgerblue", lwd = 2 )
```


 - The above three plots look much better now.
 - The scatterplot presents the linear relationship between price and carat.
 - The fitted vs residual plots shows that the residuals have equal variance and start to average to 0, which does not violate the constant variance assumption.
 - From the Q-Q plot, the points are closely aligned with the qqline, which does not violate the normality assumption.


**(e)** Use the model from part **(d)** to predict the price (in dollars) of a 3-carat diamond. Construct a 99% prediction interval for the price (in dollars).


```{r}
carat3 = data.frame(carat = 3)
exp(predict(diamonds_log_model, newdata = carat3, interval = "prediction", level = 0.99))
```


 - The 99% prediction interval for the price is (14959, 57894).
