
title: "bching3-sim-proj"
author: "Brandon Ching"
output: html_document
---

```{r include=FALSE}
library(tidyverse);
```


# Simulation Study 1
```{r}
# Set seed
birthday = 19810908;
set.seed(birthday);
```

## Introduction
The purpose of this study is to test the significance of regression against two different models. The form of both models is as follows:

$Y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3} + e_i$

where $e_i \sim N(0, \sigma^2)$

and a "significant" model having:

$\beta_0 = 3$

$\beta_1 = 1$

$\beta_2 = 1$

$\beta_3 = 1$

and a "non-significant" model having:

$\beta_0 = 3$

$\beta_1 = 0$

$\beta_2 = 0$

$\beta_3 = 0$

## Methods

To test the significance of these two models, we must first initialize the necessary $\beta$ variables, counts, and import the provided data

```{r}
# Load predictors
predictors = read_csv("study_1.csv");
# Set out betas for both models. s = significant; ns = non-significant.
beta_0_s = 3;
beta_1_s = 1;
beta_2_s = 1;
beta_3_s = 1;
beta_0_ns = 3;
beta_1_ns = 0;
beta_2_ns = 0;
beta_3_ns = 0;
# General values
n = nrow(predictors);
sigmas = c(1, 5, 10);
p = 3;
sims = 2500;
```

Next, we create the necessary tracker data frames that will hold the generated data from the simulations

```{r}
# Tracker data frames
f_stat_s = data.frame("sig 1" = rep(0, sims), "sig 5" = rep(0, sims), "sig 10" = rep(0, sims));
f_stat_ns = data.frame("sig 1" = rep(0, sims), "sig 5" = rep(0, sims), "sig 10" = rep(0, sims));
p_value_s = data.frame("sig 1" = rep(0, sims), "sig 5" = rep(0, sims), "sig 10" = rep(0, sims));
p_value_ns = data.frame("sig 1" = rep(0, sims), "sig 5" = rep(0, sims), "sig 10" = rep(0, sims));
r_2_s = data.frame("sig 1" = rep(0, sims), "sig 5" = rep(0, sims), "sig 10" = rep(0, sims));
r_2_ns = data.frame("sig 1" = rep(0, sims), "sig 5" = rep(0, sims), "sig 10" = rep(0, sims));
```

Finally, we can fit and simulate our models. Using nested for loops, we can easily iterate through all sigmas and the requisite number of simulations for each. For each simulation, we create our random error parameter based on the model. Since we need to run sumulations for both models using the same training data and $\sigma$ values, we do this in sequence. After the $y$ values are generated, we fit the model then save the F statistic, p-value, and $R^2$ for each simulation into our tracker data frame.

```{r}
# Generate y's and train
for(sig in 1:length(sigmas)){
  for(i in 1:sims){
    eps = rnorm(n, mean = 0, sd = sigmas[sig]);
    
    # Train significant model
    y = as.vector(beta_0_s + beta_1_s * predictors["x1"] + beta_2_s * predictors["x2"] + beta_3_s * predictors["x3"] + eps)[,1];
    predictors$y = y;
    local_model_s = lm(y ~ x1 + x2 + x3, data = predictors);
    local_model_summary_s = summary(local_model_s);
    
    f_stat_s[i,sig] = local_model_summary_s$fstatistic[[1]];
    r_2_s[i, sig] = local_model_summary_s$r.squared;
    p_value_s[i, sig] = pf(local_model_summary_s$fstatistic[1], local_model_summary_s$fstatistic[2], local_model_summary_s$fstatistic[3], lower.tail = FALSE)
    
    
    # Train non-significant model
    y = as.vector(beta_0_ns + beta_1_ns * predictors["x1"] + beta_2_ns * predictors["x2"] + beta_3_ns * predictors["x3"] + eps)[,1];
    predictors$y = y;
    local_model_ns = lm(y ~ x1 + x2 + x3, data = predictors);
    local_model_summary_ns = summary(local_model_ns);
    
    f_stat_ns[i, sig] = local_model_summary_ns$fstatistic[[1]];
    r_2_ns[i, sig] = local_model_summary_ns$r.squared;
    p_value_ns[i, sig] = pf(local_model_summary_ns$fstatistic[1], local_model_summary_ns$fstatistic[2], local_model_summary_ns$fstatistic[3], lower.tail = FALSE)
  }
}
```

```{r}
head(f_stat_s$sig.1)
```



## Results
### F Statistic Results
```{r}
# F statistic sig = 1
par(mfrow = c(1,2));
hist(f_stat_s$sig.1,
     main = "F Stat - S sig = 1",
     border = "blue",
     xlab = "F Statistic",
     prob = TRUE
     );
x = f_stat_s$sig.1;
curve( df(x, df1 = 3, df2 = 2497), col = "darkorange", add = TRUE, lwd = 3);
hist(f_stat_ns$sig.1,
     main = "F Stat - NS sig = 1",
     border = "blue",
     xlab = "F Statistic",
     prob = TRUE
     );
x = f_stat_ns$sig.1;
curve( df(x, df1 = 3, df2 = 2497), col = "darkorange", add = TRUE, lwd = 3);
# F statistic sig = 5
par(mfrow = c(1,2));
hist(f_stat_s$sig.5,
     main = "F Statistic - S sigma = 5",
     border = "red",
     xlab = "F Statistic",
     prob = TRUE
     );
x = f_stat_s$sig.5;
curve( df(x, df1 = 3, df2 = 2497), col = "darkorange", add = TRUE, lwd = 3);
hist(f_stat_ns$sig.5,
     main = "F Statistic - NS sigma = 5",
     border = "red",
     xlab = "F Statistic",
     prob = TRUE
     );
x = f_stat_ns$sig.5;
curve( df(x, df1 = 3, df2 = 2497), col = "darkorange", add = TRUE, lwd = 3);
# F statistic sig = 10
par(mfrow = c(1,2));
hist(f_stat_s$sig.10,
     main = "F Statistic - S sigma = 10",
     border = "green",
     xlab = "F Statistic",
     prob = TRUE
     );
x = f_stat_ns$sig.10;
curve( df(x, df1 = 3, df2 = 2497), col = "darkorange", add = TRUE, lwd = 3);
hist(f_stat_ns$sig.10,
     main = "F Statistic - NS sigma = 10",
     border = "green",
     xlab = "F Statistic",
     prob = TRUE
     );
x = f_stat_ns$sig.10;
curve( df(x, df1 = 3, df2 = 2497), col = "darkorange", add = TRUE, lwd = 3);
```

### p-value Results

```{r}
# p-value 1
par(mfrow = c(1,2));
hist(p_value_s$sig.1,
     main = "p-value - S sigma = 1",
     border = "blue",
     xlab = "p-value",
     prob = TRUE
     );
hist(p_value_ns$sig.1,
     main = "p-value - NS sigma = 1",
     border = "blue",
     xlab = "p-value",
     prob = TRUE
     );
# p-value 5
par(mfrow = c(1,2));
hist(p_value_s$sig.5,
     main = "p-value - S sigma = 5",
     border = "red",
     xlab = "p-value",
     prob = TRUE
     );
hist(p_value_ns$sig.5,
     main = "p-value - NS sigma = 5",
     border = "red",
     xlab = "p-value",
     prob = TRUE
     );
# p-valule 10
par(mfrow = c(1,2));
hist(p_value_s$sig.10,
     main = "p-value - S sigma = 10",
     border = "green",
     xlab = "p-value",
     prob = TRUE
     );
hist(p_value_ns$sig.10,
     main = "p-value - NS sigma = 10",
     border = "green",
     xlab = "p-value",
     prob = TRUE
     );
```

### R^2 Results

```{r}
# r2 1
par(mfrow = c(1,2));
hist(r_2_s$sig.1,
     main = "R^2 - S sigma = 1",
     border = "blue",
     xlab = "R^2",
     prob = TRUE
     );
hist(r_2_ns$sig.1,
     main = "R^2 - NS sigma = 1",
     border = "blue",
     xlab = "R^2",
     prob = TRUE
     );
x = r_2_ns$sig.1;
# r2 5
par(mfrow = c(1,2));
hist(r_2_s$sig.5,
     main = "R^2 - S sigma = 5",
     border = "red",
     xlab = "R^2",
     prob = TRUE
     );
hist(r_2_ns$sig.5,
     main = "R^2 - NS sigma = 5",
     border = "red",
     xlab = "R^2",
     prob = TRUE
     );
# r2 10
par(mfrow = c(1,2));
hist(r_2_s$sig.10,
     main = "R^2 - S sigma = 10",
     border = "green",
     xlab = "R^2",
     prob = TRUE
     );
hist(r_2_ns$sig.10,
     main = "R^2 - NS sigma = 10",
     border = "green",
     xlab = "R^2",
     prob = TRUE
     )
```


## Discussion
### F Statistic
We would expect that F statistic results would align with a standard F Distribution. In the above graphs, I have plotted the emperical simulation F statistic results against the curve of a true F Distribution. As we can see, the "significant" models do not exactly align with the distribution curve. This is especially true for values at $\sigma = 1$. This means that the significant model is indeed significant if its distribution does not match the curve for its distribution type since $H_0$ = distributions are the same. As $\sigma$ increases the distribution seems be become close inline with the true F Distribution curve.

### p-value
The p-value graphs for the non-significant model at all $\sigma$ values appear to be uniform distributions. The significant models at all $\sigma$ levels do not fit this distribution through as $\sigma$ increases, it does appear to get closer. I did not add a uniform line to these graphs as it seemed pretty obvious.

### R^2
I could not successfully determine the type of distribution for $R^2$ results. The non-significant model at all $\sigma$ values
appears to be similar to a F Distribution but attempts to plot this curve as I did with the F statistic values was unsuccessful. I also tried to plot Chi Square and normal curves and these did not appear correct either. 

Looking at the significant model, lower values of $\sigma$ average to higher $R^2$ values which would make sense since $\sigma$ influences the noise/error in the generation of y values. This relationship is inverse in that the lower the $\sigma$ the higher explanatory power of the model. In terms of plots for the significant model, at lower $\sigma$ values, the distribution appears normal but as $\sigma$ increases, the mean moves closer to 0 and changes the shape of the curve to align closer to the non-significant model.
