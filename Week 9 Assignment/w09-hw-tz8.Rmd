---
title: "Week 9 - Homework"
author: "STAT 420, Summer 2018, Unger"
date: ''
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
```

## Exercise 1 (`longley` Macroeconomic Data)

The built-in dataset `longley` contains macroeconomic data for predicting employment. We will attempt to model the `Employed` variable.

```{r, eval = FALSE}
View(longley)
?longley
```

**(a)** What is the largest correlation between any pair of predictors in the dataset?

```{r}
pairs(longley)
round(cor(longley), 3)
```


 - It appears that the largest correlation is between the predictors `Year` and `GNP`, which is 0.995.
 

**(b)** Fit a model with `Employed` as the response and the remaining variables as predictors. Calculate and report the variance inflation factor (VIF) for each of the predictors. Which variable has the largest VIF? Do any of the VIFs suggest multicollinearity?


```{r}
library(faraway)
employed_model = lm(Employed ~ ., data = longley)
vif(employed_model)
vif_result = vif(employed_model)

vif_result[which.max(vif_result)] #largest vif
vif_result[which(vif_result > 5)]

```


 - GNP has the largest VIF, which is 1788.513.
 - All predictors except Armed.Forces have very large VIFs, suggesting multicollinearity.


**(c)** What proportion of the observed variation in `Population` is explained by a linear relationship with the other predictors?

```{r}
popu_model = lm(Population ~ . - Employed, data = longley)
summary(popu_model)$r.squared
```


 - 99.75% of the observed variation in `Population` is explained by a linear relationship with the other predictors. 
 

**(d)** Calculate the partial correlation coefficient for `Population` and `Employed` **with the effects of the other predictors removed**.


```{r}
par_cor_model = lm(Employed ~ . - Population, data = longley)
cor(resid(popu_model), resid(par_cor_model))

```


 - The partial correlation coefficient for `Population` and `Employed` is -0.07514.
 

**(e)** Fit a new model with `Employed` as the response and the predictors from the model in **(b)** that were significant. (Use $\alpha = 0.05$.) Calculate and report the variance inflation factor for each of the predictors. Which variable has the largest VIF? Do any of the VIFs suggest multicollinearity?

```{r}
summary(employed_model)
```


 - Based on the p-values of the predictors with $\alpha = 0.05$, we choose `Unemployed`, `Armed.Forces` and `Year` as the predictors in the new model.
 
 
```{r}
employ_new_mod = lm(Employed ~ Unemployed + Armed.Forces + Year, data = longley)
vif(employ_new_mod)

vif_e = vif(employ_new_mod)
vif_e[which.max(vif_e)]
vif_e[which(vif_e > 5)]
```
 

 - The predictor `Year` has the largest VIF, which is 3.891.
 - None of the VIFs is greater than 5, so none of the VIFs suggests multicollinearity in this model.
 

**(f)** Use an $F$-test to compare the models in parts **(b)** and **(e)**. Report the following:

- The null hypothesis
- The test statistic
- The distribution of the test statistic under the null hypothesis
- The p-value
- A decision
- Which model you prefer, **(b)** or **(e)**


```{r}
anova(employ_new_mod, employed_model)
```


 - $H_0$: $\beta_{GNP.deflator}$ = $\beta_{GNP}$ = $\beta_{Population}$ = 0

 - $F$ stat = 1.75

 - Distribution: F distribution with degrees of freedom 3 and 9.

 - P-value = 0.23
 
 - Decision: Do not reject the null hypothesis.

 - Prefer: the model in **(e)** which is the smaller one.
 

**(g)** Check the assumptions of the model chosen in part **(f)**. Do any assumptions appear to be violated?

```{r, echo = FALSE}
plot_fitted_resid = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  plot(fitted(model), resid(model), 
       col = pointcol, pch = 20, cex = 1.5,
       xlab = "Fitted", ylab = "Residuals")
  abline(h = 0, col = linecol, lwd = 2)
}

plot_qq = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  qqnorm(resid(model), col = pointcol, pch = 20, cex = 1.5)
  qqline(resid(model), col = linecol, lwd = 2)
}
```

```{r}
library(lmtest)
bptest(employ_new_mod)
shapiro.test(resid(employ_new_mod))
par(mfrow = c(1, 2))
plot_fitted_resid(employ_new_mod)
plot_qq(employ_new_mod)
```


 - Based on the bptest and shapiro wilk test, both results fail to reject the null hypotheses respectively, indicating no violation of constant variance and normality assumptions. 
 - Based on the plots, there seems no violation of the assumptions as well.


***

## Exercise 2 (`Credit` Data)

For this exercise, use the `Credit` data from the `ISLR` package. Use the following code to remove the `ID` variable which is not useful for modeling.

```{r}
library(ISLR)
data(Credit)
Credit = subset(Credit, select = -c(ID))
```

Use `?Credit` to learn about this dataset.

**(a)** Find a "good" model for `balance` using the available predictors. Use any methods seen in class except transformations of the response. The model should:

- Reach a LOOCV-RMSE below `135`
- Obtain an adjusted $R^2$ above `0.90`
- Fail to reject the Breusch-Pagan test with an $\alpha$ of $0.01$
- Use fewer than 10 $\beta$ parameters

Store your model in a variable called `mod_a`. Run the two given chunks to verify your model meets the requested criteria. If you cannot find a model that meets all criteria, partial credit will be given for meeting at least some of the criteria.


```{r, fig.height=15, fig.width=20, message=FALSE, warning=FALSE}
library(MASS)
pairs(Credit, col = "dodgerblue")
```


```{r, message=FALSE, warning=FALSE}
# start with a model 
library(leaps)

credit_model_a = lm(Balance ~ Ethnicity + Married + Student + Gender + Education + Age + Cards + Rating + Student:Education + Income:Education + Income:Age + Income:Gender + Age:Student+ log(Limit) + Income + Cards:Age:Student, data = Credit)

#Backward AIC
credit_mod_back_aic = step(credit_model_a, direction = "backward", trace = 0)

#Backward BIC
n = length(resid(credit_model_a))
credit_mod_back_bic = step(credit_model_a, direction = "backward", k = log(n), trace = 0)

#exhaustive search
all_credit_mod = summary(regsubsets(Balance ~ ., data = Credit))

(best_r2_ind = which.max(all_credit_mod$adjr2))
all_credit_mod$which[best_r2_ind, ]

#best r2 model
best_r2_model = lm(Balance ~ Income + Limit + Rating + Cards + Age + Gender + Student, data = Credit)

#Choose the backward bic model as the best model

best_model = credit_mod_back_bic


#At this point, the best model passes three of the four test except bp test, so we are going to check the outliers and influential points

outliers = as.vector(as.integer(names(rstandard(best_model)[abs(rstandard(best_model)) > 2])))

influential_obs = as.vector(which(cooks.distance(best_model) > 4 / length(cooks.distance(best_model))))

#Remove outliers and influential observations from the dataset
remove = c(outliers, influential_obs)
credit_new = Credit[-remove,]

#Set the mod_a using the new dataset with the predictors stored in the credit_mod_back_bic
mod_a = lm(Balance ~ Student + Age + Rating + log(Limit) + Income, data = credit_new)



```



```{r, message = FALSE, warning = FALSE}
library(lmtest)

get_bp_decision = function(model, alpha) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_num_params = function(model) {
  length(coef(model))
}

get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}
```

```{r}
get_loocv_rmse(mod_a)
get_adj_r2(mod_a)
get_bp_decision(mod_a, alpha = 0.01)
get_num_params(mod_a)
```

 - I've tried numerous interactions and polynomials and wanted to avoid the effect from `Student` predictor for hours but failed. Then I removed the outliers and influential points, but, still, failed to reject the BP test.
 



**(b)** Find another "good" model for `balance` using the available predictors. Use any methods seen in class except transformations of the response. The model should:

- Reach a LOOCV-RMSE below `125`
- Obtain an adjusted $R^2$ above `0.91`
- Fail to reject the Shapiro-Wilk test with an $\alpha$ of $0.01$
- Use fewer than 25 $\beta$ parameters

Store your model in a variable called `mod_b`. Run the two given chunks to verify your model meets the requested criteria. If you cannot find a model that meets all criteria, partial credit will be given for meeting at least some of the criteria.


```{r, message=FALSE, warning=FALSE}
# start with a model 
library(leaps)

# Modify the model in part(a) a little bit
mod_b = lm(Balance ~ Income + Rating + log(Limit) + Student, data = Credit)
```



```{r, message = FALSE, warning = FALSE}
library(lmtest)

get_bp_decision = function(model, alpha) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_num_params = function(model) {
  length(coef(model))
}

get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}
```

```{r}
get_loocv_rmse(mod_b)
get_adj_r2(mod_b)
get_sw_decision(mod_b, alpha = 0.01)
get_num_params(mod_b)
```

 - `mod b` which takes `Income`, `Rating`, `log(Limit)`, and `Student` as its predictors meets the four requirements in part **(b)**.

***

## Exercise 3 (`Sacramento` Housing Data)

For this exercise, use the `Sacramento` data from the `caret` package. Use the following code to perform some preprocessing of the data.

```{r, message = FALSE, warning = FALSE}
library(caret)
library(ggplot2)
data(Sacramento)
sac_data = Sacramento
sac_data$limits = factor(ifelse(sac_data$city == "SACRAMENTO", "in", "out"))
sac_data = subset(sac_data, select = -c(city, zip))
```

Instead of using the `city` or `zip` variables that exist in the dataset, we will simply create a variable (`limits`) indicating whether or not a house is technically within the city limits of Sacramento. (We do this because they would both be factor variables with a **large** number of levels. This is a choice that is made due to laziness, not necessarily because it is justified. Think about what issues these variables might cause.)

Use `?Sacramento` to learn more about this dataset.

A plot of longitude versus latitude gives us a sense of where the city limits are.

```{r}
qplot(y = longitude, x = latitude, data = sac_data,
      col = limits, main = "Sacramento City Limits ")
```

After these modifications, we test-train split the data.

```{r}
set.seed(420)
sac_trn_idx  = sample(nrow(sac_data), size = trunc(0.80 * nrow(sac_data)))
sac_trn_data = sac_data[sac_trn_idx, ]
sac_tst_data = sac_data[-sac_trn_idx, ]
```

The training data should be used for all model fitting. Our goal is to find a model that is useful for predicting home prices.

**(a)** Find a "good" model for `price`. Use any methods seen in class. The model should reach a LOOCV-RMSE below 77,500 in the training data. Do not use any transformations of the response variable.


```{r}
#exhaustive method
price_model = lm(price ~ ., data = sac_trn_data)
all_price_model = summary(regsubsets(price ~ ., data = sac_trn_data))

#Find model with best r-squared
best_r2_ind = which.max(all_price_model$adjr2)
all_price_model$which[best_r2_ind, ]
#The best r-squared model
best_r2_model = lm(price ~ beds + sqft + type + latitude + longitude + limits, data = sac_trn_data)

p = length(coef(price_model))
n = length(resid(price_model))

```


```{r}
#Backward method
price_back_aic = step(price_model, direction = "backward")
```



```{r}
price_back_bic = step(price_model, direction = "backward", k = log(n))
```


```{r}
#loocv_rmse
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
#r2
r2 = function(model){
  summary(model)$r.squared
}
```

```{r}
#best_r2_model
calc_loocv_rmse(best_r2_model)
r2(best_r2_model)

#price_back_aic
calc_loocv_rmse(price_back_aic)
r2(price_back_aic)

#price_back_bic
calc_loocv_rmse(price_back_bic)
r2(price_back_bic)

```

 - Based on the output above, we are going to choose `price_back_aic` model, since it has the lowest rmse which is 77393, and it has roughly the same r2 as the `best_r2_model`.
 
 
```{r}
#The good model for price
best_price_model = price_back_aic
```
 


**(b)** Is a model that achieves a LOOCV-RMSE below 77,500 useful in this case? That is, is an average error of 77,500 low enough when predicting home prices? To further investigate, use the held-out test data and your model from part **(a)** to do two things:

- Calculate the average percent error:
\[
\frac{1}{n}\sum_i\frac{|\text{predicted}_i - \text{actual}_i|}{\text{predicted}_i} \times 100
\]
- Plot the predicted versus the actual values and add the line $y = x$.

Based on all of this information, argue whether or not this model is useful.

```{r}

sac_predict = predict(best_price_model, newdata = sac_tst_data)

n = length(resid(best_price_model))
#average percent error
avg_per_err = mean(abs(sac_predict - sac_tst_data$price) / sac_predict) * 100
avg_per_err

plot(sac_tst_data$price, 
     sac_predict, 
     col = "dodgerblue", 
     pch = 20,
     main = "Prediction vs Actual",
     xlab = "Actual",
     ylab = "Prediction"
     )
abline(0, 1, col = "darkorange")

```


 - I don't think a model achieves a LOOCV-RMSE below 77500 is useful in this case. An average error of 77,500 seems not low enough when predicting home prices. 
 - As calculated above, the average percent error is 24.83%, which is quite high. Also, the plot shows that even though the points roughly align with the trend y = x (i.e. the predictions equal to the actuals). There are still a lot of points deviated from the line, which indicated a relatively high level of average error exists.
 - Therefore, even though the model can rough predict the prices, the model is not quite useful due to the relatively high average percent error.
 - In order to improve this model, we could choose a standard of LOOCV-RMSE lower than 77500, and adjust our model in order to make the prediction more accurate.
 

***

## Exercise 4 (Does It Work?)

In this exercise, we will investigate how well backwards AIC and BIC actually perform. For either to be "working" correctly, they should result in a low number of both **false positives** and **false negatives**. In model selection,

- **False Positive**, FP: Incorrectly including a variable in the model. Including a *non-significant* variable
- **False Negative**, FN: Incorrectly excluding a variable in the model. Excluding a *significant* variable

Consider the **true** model

\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \beta_5 x_5 + \beta_6 x_6 + \beta_7 x_7 + \beta_8 x_8 + \beta_9 x_9 + \beta_{10} x_{10} + \epsilon
\]

where $\epsilon \sim N(0, \sigma^2 = 4)$. The true values of the $\beta$ parameters are given in the `R` code below.

```{r}
beta_0  = 1
beta_1  = -1
beta_2  = 2
beta_3  = -2
beta_4  = 1
beta_5  = 1
beta_6  = 0
beta_7  = 0
beta_8  = 0
beta_9  = 0
beta_10 = 0
sigma = 2
```

Then, as we have specified them, some variables are significant, and some are not. We store their names in `R` variables for use later.

```{r}
not_sig  = c("x_6", "x_7", "x_8", "x_9", "x_10")
signif = c("x_1", "x_2", "x_3", "x_4", "x_5")
```

We now simulate values for these `x` variables, which we will use throughout part **(a)**.

```{r}
set.seed(420)
n = 100
x_1  = runif(n, 0, 10)
x_2  = runif(n, 0, 10)
x_3  = runif(n, 0, 10)
x_4  = runif(n, 0, 10)
x_5  = runif(n, 0, 10)
x_6  = runif(n, 0, 10)
x_7  = runif(n, 0, 10)
x_8  = runif(n, 0, 10)
x_9  = runif(n, 0, 10)
x_10 = runif(n, 0, 10)
```

We then combine these into a data frame and simulate `y` according to the true model.

```{r}
sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
  y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + 
      beta_5 * x_5 + rnorm(n, 0 , sigma)
)
```

We do a quick check to make sure everything looks correct.

```{r}
head(sim_data_1)
```

Now, we fit an incorrect model.

```{r}
fit = lm(y ~ x_1 + x_2 + x_6 + x_7, data = sim_data_1)
coef(fit)
```

Notice, we have coefficients for `x_1`, `x_2`, `x_6`, and `x_7`. This means that `x_6` and `x_7` are false positives, while `x_3`, `x_4`, and `x_5` are false negatives.

To detect the false negatives, use:

```{r}
# which are false negatives?
!(signif %in% names(coef(fit)))
```

To detect the false positives, use:

```{r}
# which are false positives?
names(coef(fit)) %in% not_sig
```

Note that in both cases, you could `sum()` the result to obtain the number of false negatives or positives.

**(a)** Set a seed equal to your birthday; then, using the given data for each `x` variable above in `sim_data_1`, simulate the response variable `y` 300 times. Each time,

- Fit an additive model using each of the `x` variables.
- Perform variable selection using backwards AIC.
- Perform variable selection using backwards BIC.
- Calculate and store the number of false negatives for the models chosen by AIC and BIC.
- Calculate and store the number of false positives for the models chosen by AIC and BIC.

Calculate the rate of false positives and negatives for both AIC and BIC. Compare the rates between the two methods. Arrange your results in a well formatted table.

```{r}
set.seed(19980121)
num_sim = 300
f_posi_aic = rep(0, num_sim)
f_nega_aic = rep(0, num_sim)
f_posi_bic = rep(0, num_sim)
f_nega_bic = rep(0, num_sim)

for(i in 1:num_sim){
  sim_data_1$y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + beta_5 * x_5 + rnorm(n, 0 , sigma)
  
  fit = lm(y ~ ., data = sim_data_1)
  fit_backward_aic = step(fit, direction = "backward", trace = 0)
 
  n = length(resid(fit))
  fit_backward_bic = step(fit, direction = "backward", k = log(n), trace = 0)
  
  f_posi_aic[i] = sum(names(coef(fit_backward_aic)) %in% not_sig)
  f_nega_aic[i] = sum(!(signif %in% names(coef(fit_backward_aic))))
  f_posi_bic[i] = sum(names(coef(fit_backward_bic)) %in% not_sig)
  f_nega_bic[i] = sum(!(signif %in% names(coef(fit_backward_bic))))
}
```

```{r}
mean(f_posi_aic)
mean(f_nega_aic)
mean(f_posi_bic)
mean(f_nega_bic)
```

```{r}
#Arrange the results
library(knitr)
table = data.frame(
  "AIC method" = c(
    "False Negatives" = mean(f_nega_aic),
    "False Positives" = mean(f_posi_aic)
  ),
  "BIC method" = c(
    "False Negatives" = mean(f_nega_bic),
    "False Positives" = mean(f_posi_bic)
  )
)
kable(table)
```


 - As can see from the above table, for both AIC and BIC method, no false negatives are produced, and they only have false positives.
 - The amount of False Positives that BIC generates are less than the amount of False Positives that AIC generates. This might because BIC has a more strict restriction on the model size.
 

**(b)** Set a seed equal to your birthday; then, using the given data for each `x` variable below in `sim_data_2`, simulate the response variable `y` 300 times. Each time,

- Fit an additive model using each of the `x` variables.
- Perform variable selection using backwards AIC.
- Perform variable selection using backwards BIC.
- Calculate and store the number of false negatives for the models chosen by AIC and BIC.
- Calculate and store the number of false positives for the models chosen by AIC and BIC.

Calculate the rate of false positives and negatives for both AIC and BIC. Compare the rates between the two methods. Arrange your results in a well formatted table. Also compare to your answers in part **(a)** and suggest a reason for any differences.

```{r}
set.seed(19980121)
x_1  = runif(n, 0, 10)
x_2  = runif(n, 0, 10)
x_3  = runif(n, 0, 10)
x_4  = runif(n, 0, 10)
x_5  = runif(n, 0, 10)
x_6  = runif(n, 0, 10)
x_7  = runif(n, 0, 10)
x_8  = x_1 + rnorm(n, 0, 0.1)
x_9  = x_1 + rnorm(n, 0, 0.1)
x_10 = x_2 + rnorm(n, 0, 0.1)

sim_data_2 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
  y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + 
      beta_5 * x_5 + rnorm(n, 0 , sigma)
)
```

```{r}
set.seed(19980121)
num_sim = 300
f_nega_aic = rep(0, num_sim)
f_posi_aic = rep(0, num_sim)
f_nega_bic = rep(0, num_sim)
f_posi_bic = rep(0, num_sim)


for(i in 1:num_sim){
  sim_data_2$y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + beta_5 * x_5 + rnorm(n, 0 , sigma)
  
  fit = lm(y ~ ., data = sim_data_2)
  fit_backward_aic = step(fit, direction = "backward", trace = 0)
 
  n = length(resid(fit))
  fit_backward_bic = step(fit, direction = "backward", k = log(n), trace = 0)
  
  f_posi_aic[i] = sum(names(coef(fit_backward_aic)) %in% not_sig)
  f_nega_aic[i] = sum(!(signif %in% names(coef(fit_backward_aic))))
  f_posi_bic[i] = sum(names(coef(fit_backward_bic)) %in% not_sig)
  f_nega_bic[i] = sum(!(signif %in% names(coef(fit_backward_bic))))
}

```

```{r}
mean(f_posi_aic)
mean(f_nega_aic)
mean(f_posi_bic)
mean(f_nega_bic)
```

```{r}
#Arrange the results
library(knitr)
table = data.frame(
  "AIC method" = c(
    "False Negatives" = mean(f_nega_aic),
    "False Positives" = mean(f_posi_aic)
  ),
  "BIC method" = c(
    "False Negatives" = mean(f_nega_bic),
    "False Positives" = mean(f_posi_bic)
  )
)
kable(table)
```



 - Based on the output in the table above, BIC gives less false positives than AIC. However, in part **(b)**, they have false negatives under both methods, while they have no false negatives in part **(a)**. The difference may be caused by the correlation among the predictors.
 
 
```{r}
#part A correlation among predictors
cor(sim_data_1)

#part B correlation among predictors
cor(sim_data_2)
```
 
 
 
  - By comparing the two correlation tables, we found that the predictors of `sim_data_1` in part **(a)** are much less correlated than the predictors of `sim_data_2`. 
  - Predictors in part **(a)** could be considered as somewhat independent, since the magnitude of the correlations are less than 0.20.
  - As we can see in the correlation table of part **(b)**, there are extremely high correlation between x2 and x10, x1 and x9, x1 and x8, and x9 and x8, whose correlations are around 0.99.
  
 